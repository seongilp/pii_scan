import json
import argparse
from datetime import timedelta

def analyze_scan_preview(file_path: str):
    """
    ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡° ë¶„ì„ JSON íŒŒì¼ì„ ì½ì–´ ìš”ì•½ ì •ë³´ë¥¼ ì¶œë ¥í•˜ê³ ,
    ì²˜ë¦¬ ë¹„ìš©ì´ ë†’ì€ í…Œì´ë¸”ì„ ì‹ë³„í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        file_path (str): ë¶„ì„í•  JSON íŒŒì¼ì˜ ê²½ë¡œ
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            analyses = json.load(f)
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        print("ğŸ’¡ íŒŒì¼ ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    except json.JSONDecodeError:
        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {file_path} íŒŒì¼ì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return

    print("=" * 80)
    print("ğŸ“Š Polars ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡° ë¶„ì„ ë¦¬í¬íŠ¸")
    print(f"ğŸ“„ ë¶„ì„ íŒŒì¼: {file_path}")
    print("=" * 80)

    # 1. ì „ì²´ í†µê³„ ìš”ì•½
    total_db_count = len(analyses)
    total_summary = {
        'tables': sum(db.get('summary', {}).get('total_tables', 0) for db in analyses),
        'rows': sum(db.get('summary', {}).get('total_rows', 0) for db in analyses),
        'columns': sum(db.get('summary', {}).get('total_columns', 0) for db in analyses),
        'scannable_tables': sum(db.get('summary', {}).get('scannable_tables', 0) for db in analyses),
        'large_tables': sum(db.get('summary', {}).get('large_tables', 0) for db in analyses),
        'est_mb': sum(db.get('summary', {}).get('estimated_total_mb', 0) for db in analyses),
        'est_time_sec': sum(db.get('summary', {}).get('estimated_total_scan_time_sec', 0) for db in analyses),
    }

    print("ğŸ“ˆ ì „ì²´ ê·œëª¨ ìš”ì•½")
    print(f"  â€¢ ë°ì´í„°ë² ì´ìŠ¤: {total_db_count}ê°œ")
    print(f"  â€¢ ì´ í…Œì´ë¸”: {total_summary['tables']:,}ê°œ (ìŠ¤ìº” ê°€ëŠ¥: {total_summary['scannable_tables']:,}ê°œ)")
    print(f"  â€¢ ì´ í–‰ ìˆ˜: {total_summary['rows']:,}í–‰")
    print(f"  â€¢ ì´ ì»¬ëŸ¼ ìˆ˜: {total_summary['columns']:,}ê°œ")
    print(f"  â€¢ ëŒ€ìš©ëŸ‰ í…Œì´ë¸” (100ë§Œ í–‰ ì´ìƒ): {total_summary['large_tables']:,}ê°œ")
    print("-" * 40)
    
    # 2. ì˜ˆìƒ ì²˜ë¦¬ ë¹„ìš© ë¶„ì„
    est_time = timedelta(seconds=int(total_summary['est_time_sec']))
    print("ğŸ’¾ ì˜ˆìƒ ì²˜ë¦¬ ë¹„ìš© (Polars ì—”ì§„)")
    print(f"  â€¢ ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ìƒ˜í”Œë§ ê¸°ë°˜): {total_summary['est_mb']:.2f} MB")
    print(f"  â€¢ ì˜ˆìƒ ì´ ìŠ¤ìº” ì‹œê°„: {str(est_time)} ({total_summary['est_time_sec']:.2f}ì´ˆ)")
    print("-" * 80)

    # 3. ë¹„ìš© ë†’ì€ í…Œì´ë¸” ì‹ë³„
    high_cost_tables = []
    for db in analyses:
        db_name = db.get('database', 'Unknown')
        for table_name, table_data in db.get('tables', {}).items():
            is_large = table_data.get('total_rows', 0) >= 1000000
            is_slow = table_data.get('time_estimate', {}).get('total_estimated_sec', 0) > 30
            
            if is_large or is_slow:
                high_cost_tables.append({
                    'db': db_name,
                    'table': table_name,
                    'rows': table_data.get('total_rows', 0),
                    'cols': table_data.get('total_columns', 0),
                    'mb': table_data.get('size_estimate', {}).get('estimated_mb', 0),
                    'time_sec': table_data.get('time_estimate', {}).get('total_estimated_sec', 0),
                    'reason': "ëŒ€ìš©ëŸ‰" if is_large and not is_slow else "ì²˜ë¦¬ ì‹œê°„ ê¹€" if is_slow and not is_large else "ëŒ€ìš©ëŸ‰ & ì²˜ë¦¬ ì‹œê°„ ê¹€"
                })

    if high_cost_tables:
        print("âš ï¸  ì£¼ìš” ê²€í†  ëŒ€ìƒ í…Œì´ë¸” (ìƒìœ„ 15ê°œ)")
        print("   (í–‰ ìˆ˜ê°€ ë§ê±°ë‚˜, ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ì´ ê¸´ í…Œì´ë¸”)")
        
        # í–‰ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 15ê°œë§Œ í‘œì‹œ
        sorted_tables = sorted(high_cost_tables, key=lambda x: x['rows'], reverse=True)
        
        # í—¤ë” ì¶œë ¥
        print("-" * 80)
        print(f"  {'#':<3} {'Database':<20} {'Table':<25} {'Rows':>12} {'Time (sec)':>12}")
        print("-" * 80)

        for i, t in enumerate(sorted_tables[:15], 1):
            print(f"  {i:<3} {t['db']:<20} {t['table']:<25} {t['rows']:,>12} {t['time_sec']:>12.2f}")
        
        if len(sorted_tables) > 15:
            print(f"  ... ì™¸ {len(sorted_tables) - 15}ê°œì˜ í…Œì´ë¸”ì´ ë” ìˆìŠµë‹ˆë‹¤.")
    else:
        print("âœ… íŠ¹ë³„íˆ ì£¼ì˜ê°€ í•„ìš”í•œ ëŒ€ìš©ëŸ‰ ë˜ëŠ” ì²˜ë¦¬ ì‹œê°„ì´ ê¸´ í…Œì´ë¸”ì€ ì—†ìŠµë‹ˆë‹¤.")
    print("-" * 80)
    
    # 4. ìŠ¤ìº” ì‹¤í–‰ ê¶Œì¥ì‚¬í•­
    print("ğŸ’¡ ìŠ¤ìº” ì‹¤í–‰ ê¶Œì¥ì‚¬í•­")
    
    # ë©”ëª¨ë¦¬ ìœ„í—˜ë„ í‰ê°€
    if total_summary['est_mb'] > 1024:
        print(f"  ğŸ”´ [ë©”ëª¨ë¦¬] ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰({total_summary['est_mb']:.1f}MB)ì´ ë†’ìŠµë‹ˆë‹¤. ì„œë²„ ì‚¬ì–‘ì„ í™•ì¸í•˜ê±°ë‚˜ ìƒ˜í”Œ í¬ê¸°ë¥¼ ì¤„ì´ì„¸ìš”.")
    elif total_summary['est_mb'] > 512:
        print(f"  ğŸŸ¡ [ë©”ëª¨ë¦¬] ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰({total_summary['est_mb']:.1f}MB)ì´ ë‹¤ì†Œ ë†’ìŠµë‹ˆë‹¤. ì‹¤í–‰ ì‹œ ëª¨ë‹ˆí„°ë§ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        print(f"  ğŸŸ¢ [ë©”ëª¨ë¦¬] ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰({total_summary['est_mb']:.1f}MB)ì€ ì•ˆì •ì ì¸ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")

    # ì‹œê°„ ìœ„í—˜ë„ í‰ê°€
    if total_summary['est_time_sec'] > 1800: # 30ë¶„
        print(f"  ğŸ”´ [ì‹œê°„] ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„({str(est_time)})ì´ ë§¤ìš° ê¹ë‹ˆë‹¤. ì¤‘ìš”í•œ DBë¶€í„° ë‚˜ëˆ„ì–´ ì‹¤í–‰í•˜ê±°ë‚˜, ëŒ€ìš©ëŸ‰ í…Œì´ë¸”ì„ ì œì™¸í•˜ê³  ìŠ¤ìº”í•˜ì„¸ìš”.")
    elif total_summary['est_time_sec'] > 600: # 10ë¶„
        print(f"  ğŸŸ¡ [ì‹œê°„] ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„({str(est_time)})ì´ ë‹¤ì†Œ ê¹ë‹ˆë‹¤. ì—…ë¬´ ì˜í–¥ì´ ì ì€ ì‹œê°„ì— ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
    else:
        print(f"  ğŸŸ¢ [ì‹œê°„] ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„({str(est_time)})ì€ ì–‘í˜¸í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")

    if not high_cost_tables and total_summary['est_mb'] <= 512 and total_summary['est_time_sec'] <= 600:
        print("\nâœ… ì „ì²´ì ìœ¼ë¡œ ì²˜ë¦¬ ë¹„ìš©ì´ ë‚®ì•„ ë³´ì…ë‹ˆë‹¤. ì „ì²´ ìŠ¤ìº”ì„ ë°”ë¡œ ì§„í–‰í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.")

    print("=" * 80)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Polars DB êµ¬ì¡° ë¶„ì„(.json) íŒŒì¼ì„ ì½ê³  ìš”ì•½ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        "file_path",
        type=str,
        help="ë¶„ì„í•  polars_db_analysis_{timestamp}.json íŒŒì¼ì˜ ê²½ë¡œ"
    )
    
    args = parser.parse_args()
    analyze_scan_preview(args.file_path) 